{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Eutrophication Using Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishing data based on previous EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda_tmp\\ipykernel_18472\\3214712678.py:5: DtypeWarning: Columns (5,8,10,12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Productivity_df = pd.read_csv('Productivity_Data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lake_id</th>\n",
       "      <th>date</th>\n",
       "      <th>secchi</th>\n",
       "      <th>perag</th>\n",
       "      <th>perdev</th>\n",
       "      <th>perwater</th>\n",
       "      <th>perfor</th>\n",
       "      <th>perwet</th>\n",
       "      <th>pergrass</th>\n",
       "      <th>pershrub</th>\n",
       "      <th>depth</th>\n",
       "      <th>doc_mgl</th>\n",
       "      <th>chla_ugl</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>mean_depth_m</th>\n",
       "      <th>res_time_days</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>wshd_area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>1</td>\n",
       "      <td>2002-03-04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>1</td>\n",
       "      <td>2002-03-04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>1</td>\n",
       "      <td>2002-03-04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>1</td>\n",
       "      <td>2002-03-04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>1</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lake_id        date  secchi  perag  perdev  perwater  perfor  perwet  \\\n",
       "2325        1  2002-03-04     3.8   0.84   48.97       2.2   41.23     2.0   \n",
       "2330        1  2002-03-04     3.8   0.84   48.97       2.2   41.23     2.0   \n",
       "2332        1  2002-03-04     3.8   0.84   48.97       2.2   41.23     2.0   \n",
       "2337        1  2002-03-04     3.8   0.84   48.97       2.2   41.23     2.0   \n",
       "2341        1  2002-04-01     4.3   0.84   48.97       2.2   41.23     2.0   \n",
       "\n",
       "      pergrass  pershrub  depth  doc_mgl  chla_ugl  area_km2  mean_depth_m  \\\n",
       "2325      0.76      3.76    0.8      2.8       4.5     83.77          34.0   \n",
       "2330      0.76      3.76   59.4      2.9       2.8     83.77          34.0   \n",
       "2332      0.76      3.76    0.8      2.8       4.5     83.77          34.0   \n",
       "2337      0.76      3.76   59.4      2.9       2.8     83.77          34.0   \n",
       "2341      0.76      3.76    1.0      3.0      16.1     83.77          34.0   \n",
       "\n",
       "      res_time_days  elevation_m  wshd_area_km2  \n",
       "2325          611.1          1.0         1488.1  \n",
       "2330          611.1          1.0         1488.1  \n",
       "2332          611.1          1.0         1488.1  \n",
       "2337          611.1          1.0         1488.1  \n",
       "2341          611.1          1.0         1488.1  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "\n",
    "Secchi_df = pd.read_csv('Secchi_Data.csv')\n",
    "Use_df = pd.read_csv('Use_Data.csv')\n",
    "Productivity_df = pd.read_csv('Productivity_Data.csv')\n",
    "Hydrolakes_df = pd.read_csv('Hydrolakes_Data.csv')\n",
    "\n",
    "# Dropping the columns we don't care about\n",
    "Secchi_df = Secchi_df.drop(columns=['row_id', 'name', 'meas_location', 'sec_flag', 'year'])\n",
    "Use_df = Use_df.drop(columns=['row_id', 'name', 'comment'])\n",
    "Productivity_df = Productivity_df.drop(columns=['row_id', 'name', 'stratum', 'meas_location', 'tp_ugl', 'tp_flag', 'tn_ugl', 'tn_flag', 'doc_flag', 'chla_flag', 'comment', 'year'])\n",
    "Hydrolakes_df = Hydrolakes_df.drop(columns=['row_id', 'name', 'hylak_id', 'shore_len_km', 'dis_avg_ms3'])\n",
    "\n",
    "# Make a single dataframe\n",
    "\n",
    "df = Secchi_df.merge(Use_df, on='lake_id', how='outer') \\\n",
    "              .merge(Productivity_df, on=['lake_id', 'date'], how='outer') \\\n",
    "              .merge(Hydrolakes_df, on=['lake_id'], how='outer')\n",
    "              \n",
    "\n",
    "# Cleaning the dataset\n",
    "df = df.dropna(subset=['secchi', 'perag', 'depth', 'area_km2', 'doc_mgl', 'chla_ugl'])\n",
    "\n",
    "# Displa head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataset was used to show the effects of climate change on dissolved oxygen in lakes, the conclusion of the inital data collection being that there was a reduction over time. That conclusion was drawn from disolved organic carbon among other factors looked at in this model increasing over time. To train a model that determines the effect of farmland on this data we therefore have to decide what to do with the time series nature of the model. \n",
    "\n",
    "Our options are\n",
    "- Ignore the date\n",
    "- Use most recent datapoints only\n",
    "- Average values for each lake\n",
    "\n",
    "The exploratory data analysis revealed that the increase over all lakes seems to be relatively small in terms of total change. Note that the slope is next to zero because we are working with a very small value over a long period of time and some lakes cause an offset. Check out individual lakes to better see the increase in the total ammount of disolved organic carbon. Percent change and variability are actually what scientists are worried about with the effects of climate change but for the sake of the model thats not exactly what we care about. We care about variablitiy in that outliers and the increase in how erratic the data is holds importance for the model. Some lakes dip below their original values or have massive random spikes; we can't just grab the most recent data or average over time because it is important for the model to capture a lakes variability in order to build a more comprehensive model. Thus, it makes the most sense to drop the date column and the lake_id column to make a more general purpose model that can make estimates for any lake it is given data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>secchi</th>\n",
       "      <th>perag</th>\n",
       "      <th>perdev</th>\n",
       "      <th>perwater</th>\n",
       "      <th>perfor</th>\n",
       "      <th>perwet</th>\n",
       "      <th>pergrass</th>\n",
       "      <th>pershrub</th>\n",
       "      <th>depth</th>\n",
       "      <th>doc_mgl</th>\n",
       "      <th>chla_ugl</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>mean_depth_m</th>\n",
       "      <th>res_time_days</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>wshd_area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>4.3</td>\n",
       "      <td>0.84</td>\n",
       "      <td>48.97</td>\n",
       "      <td>2.2</td>\n",
       "      <td>41.23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>83.77</td>\n",
       "      <td>34.0</td>\n",
       "      <td>611.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1488.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      secchi  perag  perdev  perwater  perfor  perwet  pergrass  pershrub  \\\n",
       "2325     3.8   0.84   48.97       2.2   41.23     2.0      0.76      3.76   \n",
       "2330     3.8   0.84   48.97       2.2   41.23     2.0      0.76      3.76   \n",
       "2332     3.8   0.84   48.97       2.2   41.23     2.0      0.76      3.76   \n",
       "2337     3.8   0.84   48.97       2.2   41.23     2.0      0.76      3.76   \n",
       "2341     4.3   0.84   48.97       2.2   41.23     2.0      0.76      3.76   \n",
       "\n",
       "      depth  doc_mgl  chla_ugl  area_km2  mean_depth_m  res_time_days  \\\n",
       "2325    0.8      2.8       4.5     83.77          34.0          611.1   \n",
       "2330   59.4      2.9       2.8     83.77          34.0          611.1   \n",
       "2332    0.8      2.8       4.5     83.77          34.0          611.1   \n",
       "2337   59.4      2.9       2.8     83.77          34.0          611.1   \n",
       "2341    1.0      3.0      16.1     83.77          34.0          611.1   \n",
       "\n",
       "      elevation_m  wshd_area_km2  \n",
       "2325          1.0         1488.1  \n",
       "2330          1.0         1488.1  \n",
       "2332          1.0         1488.1  \n",
       "2337          1.0         1488.1  \n",
       "2341          1.0         1488.1  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['lake_id','date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7377"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function, mean absoulte error modified to bias towards enviornmentalism\n",
    "\n",
    "def custom_loss(delta): # Delta should be > 1 to punish more\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred \n",
    "        is_below_margin = error < 0 # If we predict a higher secchi value, we want to punish it more\n",
    "        #small_error_loss = K.abs(error)\n",
    "        #large_error_loss = delta * K.abs(error)\n",
    "        small_error_loss = K.square(error)\n",
    "        large_error_loss = delta * K.square(error)\n",
    "        return K.mean(K.switch(is_below_margin, large_error_loss, small_error_loss), axis=-1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preston Waters\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.9896 - val_loss: 3.6984\n",
      "Epoch 2/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4815 - val_loss: 3.5612\n",
      "Epoch 3/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4456 - val_loss: 3.5628\n",
      "Epoch 4/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4773 - val_loss: 3.6952\n",
      "Epoch 5/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3766 - val_loss: 3.4914\n",
      "Epoch 6/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2459 - val_loss: 3.5097\n",
      "Epoch 7/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5111 - val_loss: 3.5246\n",
      "Epoch 8/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5588 - val_loss: 3.4291\n",
      "Epoch 9/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2728 - val_loss: 3.5707\n",
      "Epoch 10/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3755 - val_loss: 3.3822\n",
      "Epoch 11/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3182 - val_loss: 3.4100\n",
      "Epoch 12/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3221 - val_loss: 3.3388\n",
      "Epoch 13/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2837 - val_loss: 3.3791\n",
      "Epoch 14/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2237 - val_loss: 3.4831\n",
      "Epoch 15/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2826 - val_loss: 3.3586\n",
      "Epoch 16/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2413 - val_loss: 3.3444\n",
      "Epoch 17/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2191 - val_loss: 3.3370\n",
      "Epoch 18/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3204 - val_loss: 3.4178\n",
      "Epoch 19/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2934 - val_loss: 3.3407\n",
      "Epoch 20/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2159 - val_loss: 3.2952\n",
      "Epoch 21/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2536 - val_loss: 3.2891\n",
      "Epoch 22/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2542 - val_loss: 3.2743\n",
      "Epoch 23/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1998 - val_loss: 3.2137\n",
      "Epoch 24/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0773 - val_loss: 3.3020\n",
      "Epoch 25/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3087 - val_loss: 3.1804\n",
      "Epoch 26/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2049 - val_loss: 3.1888\n",
      "Epoch 27/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2979 - val_loss: 3.1709\n",
      "Epoch 28/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9445 - val_loss: 3.1998\n",
      "Epoch 29/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2312 - val_loss: 3.0906\n",
      "Epoch 30/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0226 - val_loss: 3.2748\n",
      "Epoch 31/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2390 - val_loss: 3.1940\n",
      "Epoch 32/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0398 - val_loss: 3.1888\n",
      "Epoch 33/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9368 - val_loss: 3.0428\n",
      "Epoch 34/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0621 - val_loss: 3.0064\n",
      "Epoch 35/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9856 - val_loss: 3.0089\n",
      "Epoch 36/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9673 - val_loss: 3.0992\n",
      "Epoch 37/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9591 - val_loss: 3.0327\n",
      "Epoch 38/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0464 - val_loss: 3.0579\n",
      "Epoch 39/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0846 - val_loss: 2.9602\n",
      "Epoch 40/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0919 - val_loss: 3.0171\n",
      "Epoch 41/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9580 - val_loss: 2.9588\n",
      "Epoch 42/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8714 - val_loss: 3.0774\n",
      "Epoch 43/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9386 - val_loss: 3.0167\n",
      "Epoch 44/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9274 - val_loss: 3.1452\n",
      "Epoch 45/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0303 - val_loss: 3.0277\n",
      "Epoch 46/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8463 - val_loss: 3.1161\n",
      "Epoch 47/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8586 - val_loss: 2.9427\n",
      "Epoch 48/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9736 - val_loss: 2.9509\n",
      "Epoch 49/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9726 - val_loss: 3.0264\n",
      "Epoch 50/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8442 - val_loss: 2.9779\n",
      "Epoch 1/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.5680 - val_loss: 3.4555\n",
      "Epoch 2/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5232 - val_loss: 3.3726\n",
      "Epoch 3/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4734 - val_loss: 3.4218\n",
      "Epoch 4/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3232 - val_loss: 3.4326\n",
      "Epoch 5/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3446 - val_loss: 3.2644\n",
      "Epoch 6/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4541 - val_loss: 3.2915\n",
      "Epoch 7/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6139 - val_loss: 3.2809\n",
      "Epoch 8/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3371 - val_loss: 3.3091\n",
      "Epoch 9/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3710 - val_loss: 3.2464\n",
      "Epoch 10/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4216 - val_loss: 3.2748\n",
      "Epoch 11/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4756 - val_loss: 3.2545\n",
      "Epoch 12/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3214 - val_loss: 3.4668\n",
      "Epoch 13/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3183 - val_loss: 3.2339\n",
      "Epoch 14/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3203 - val_loss: 3.1920\n",
      "Epoch 15/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2330 - val_loss: 3.1944\n",
      "Epoch 16/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2612 - val_loss: 3.2206\n",
      "Epoch 17/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1411 - val_loss: 3.2562\n",
      "Epoch 18/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2484 - val_loss: 3.1833\n",
      "Epoch 19/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1960 - val_loss: 3.1487\n",
      "Epoch 20/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2367 - val_loss: 3.1592\n",
      "Epoch 21/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2812 - val_loss: 3.1722\n",
      "Epoch 22/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2216 - val_loss: 3.1963\n",
      "Epoch 23/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1450 - val_loss: 3.1019\n",
      "Epoch 24/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1094 - val_loss: 3.0493\n",
      "Epoch 25/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1409 - val_loss: 3.1346\n",
      "Epoch 26/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0341 - val_loss: 3.0404\n",
      "Epoch 27/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0896 - val_loss: 3.0787\n",
      "Epoch 28/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1240 - val_loss: 3.0433\n",
      "Epoch 29/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0951 - val_loss: 3.0557\n",
      "Epoch 30/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8737 - val_loss: 3.0585\n",
      "Epoch 31/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0722 - val_loss: 3.0095\n",
      "Epoch 32/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9636 - val_loss: 3.0889\n",
      "Epoch 33/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0992 - val_loss: 3.1382\n",
      "Epoch 34/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0173 - val_loss: 3.0104\n",
      "Epoch 35/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9590 - val_loss: 2.9992\n",
      "Epoch 36/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0092 - val_loss: 3.0078\n",
      "Epoch 37/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8993 - val_loss: 2.9847\n",
      "Epoch 38/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8693 - val_loss: 3.0807\n",
      "Epoch 39/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8594 - val_loss: 2.9827\n",
      "Epoch 40/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0603 - val_loss: 2.9424\n",
      "Epoch 41/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0157 - val_loss: 2.9989\n",
      "Epoch 42/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9516 - val_loss: 2.9691\n",
      "Epoch 43/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9729 - val_loss: 3.1169\n",
      "Epoch 44/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9355 - val_loss: 2.9672\n",
      "Epoch 45/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8721 - val_loss: 3.0267\n",
      "Epoch 46/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9575 - val_loss: 2.9697\n",
      "Epoch 47/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9414 - val_loss: 2.9601\n",
      "Epoch 48/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9153 - val_loss: 3.0163\n",
      "Epoch 49/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9673 - val_loss: 2.9662\n",
      "Epoch 50/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8557 - val_loss: 2.9585\n",
      "Epoch 1/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.5101 - val_loss: 3.4896\n",
      "Epoch 2/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4079 - val_loss: 3.2746\n",
      "Epoch 3/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3414 - val_loss: 3.2558\n",
      "Epoch 4/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3705 - val_loss: 3.3428\n",
      "Epoch 5/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5211 - val_loss: 3.2517\n",
      "Epoch 6/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4549 - val_loss: 3.2797\n",
      "Epoch 7/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2398 - val_loss: 3.2315\n",
      "Epoch 8/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4491 - val_loss: 3.3142\n",
      "Epoch 9/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4059 - val_loss: 3.2656\n",
      "Epoch 10/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4141 - val_loss: 3.2111\n",
      "Epoch 11/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3821 - val_loss: 3.2092\n",
      "Epoch 12/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5827 - val_loss: 3.2276\n",
      "Epoch 13/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1716 - val_loss: 3.2739\n",
      "Epoch 14/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3212 - val_loss: 3.1967\n",
      "Epoch 15/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4833 - val_loss: 3.1548\n",
      "Epoch 16/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3549 - val_loss: 3.2302\n",
      "Epoch 17/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2238 - val_loss: 3.1157\n",
      "Epoch 18/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3980 - val_loss: 3.1651\n",
      "Epoch 19/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1085 - val_loss: 3.1212\n",
      "Epoch 20/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4644 - val_loss: 3.0727\n",
      "Epoch 21/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2461 - val_loss: 3.0834\n",
      "Epoch 22/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0763 - val_loss: 3.1023\n",
      "Epoch 23/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1080 - val_loss: 3.0032\n",
      "Epoch 24/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2071 - val_loss: 3.1238\n",
      "Epoch 25/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9998 - val_loss: 3.0399\n",
      "Epoch 26/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1892 - val_loss: 3.0636\n",
      "Epoch 27/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2124 - val_loss: 2.9876\n",
      "Epoch 28/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1359 - val_loss: 2.9855\n",
      "Epoch 29/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1389 - val_loss: 2.9423\n",
      "Epoch 30/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0627 - val_loss: 3.0009\n",
      "Epoch 31/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1109 - val_loss: 2.8803\n",
      "Epoch 32/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9841 - val_loss: 2.9365\n",
      "Epoch 33/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0977 - val_loss: 2.9521\n",
      "Epoch 34/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9933 - val_loss: 2.9774\n",
      "Epoch 35/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0685 - val_loss: 2.9208\n",
      "Epoch 36/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0428 - val_loss: 2.9012\n",
      "Epoch 37/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9994 - val_loss: 3.0961\n",
      "Epoch 38/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0441 - val_loss: 2.9223\n",
      "Epoch 39/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9205 - val_loss: 2.8726\n",
      "Epoch 40/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9416 - val_loss: 2.9092\n",
      "Epoch 41/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0319 - val_loss: 2.8678\n",
      "Epoch 42/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9466 - val_loss: 2.9109\n",
      "Epoch 43/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9825 - val_loss: 2.9253\n",
      "Epoch 44/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9548 - val_loss: 2.8761\n",
      "Epoch 45/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9413 - val_loss: 2.8445\n",
      "Epoch 46/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0032 - val_loss: 2.9419\n",
      "Epoch 47/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9445 - val_loss: 2.8263\n",
      "Epoch 48/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0547 - val_loss: 2.8003\n",
      "Epoch 49/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9079 - val_loss: 2.8824\n",
      "Epoch 50/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9391 - val_loss: 2.8833\n",
      "Epoch 1/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.3913 - val_loss: 3.6316\n",
      "Epoch 2/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6178 - val_loss: 3.5138\n",
      "Epoch 3/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3674 - val_loss: 3.3926\n",
      "Epoch 4/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4290 - val_loss: 3.4133\n",
      "Epoch 5/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3922 - val_loss: 3.3176\n",
      "Epoch 6/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4961 - val_loss: 3.3642\n",
      "Epoch 7/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3173 - val_loss: 3.4605\n",
      "Epoch 8/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3202 - val_loss: 3.3213\n",
      "Epoch 9/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4186 - val_loss: 3.2520\n",
      "Epoch 10/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2983 - val_loss: 3.2357\n",
      "Epoch 11/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3285 - val_loss: 3.2689\n",
      "Epoch 12/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3496 - val_loss: 3.3316\n",
      "Epoch 13/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3005 - val_loss: 3.3065\n",
      "Epoch 14/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2819 - val_loss: 3.2397\n",
      "Epoch 15/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2937 - val_loss: 3.1972\n",
      "Epoch 16/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2280 - val_loss: 3.2340\n",
      "Epoch 17/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2119 - val_loss: 3.2540\n",
      "Epoch 18/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1775 - val_loss: 3.2425\n",
      "Epoch 19/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2352 - val_loss: 3.1700\n",
      "Epoch 20/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1148 - val_loss: 3.1443\n",
      "Epoch 21/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0782 - val_loss: 3.1119\n",
      "Epoch 22/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1694 - val_loss: 3.0088\n",
      "Epoch 23/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9983 - val_loss: 3.0518\n",
      "Epoch 24/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1067 - val_loss: 3.0075\n",
      "Epoch 25/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0290 - val_loss: 2.9778\n",
      "Epoch 26/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1286 - val_loss: 3.0024\n",
      "Epoch 27/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9749 - val_loss: 3.0546\n",
      "Epoch 28/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2180 - val_loss: 3.0510\n",
      "Epoch 29/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8827 - val_loss: 2.9966\n",
      "Epoch 30/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1168 - val_loss: 2.9852\n",
      "Epoch 31/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0301 - val_loss: 2.9565\n",
      "Epoch 32/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1091 - val_loss: 3.0281\n",
      "Epoch 33/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1633 - val_loss: 2.9395\n",
      "Epoch 34/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1698 - val_loss: 2.8882\n",
      "Epoch 35/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0574 - val_loss: 3.0233\n",
      "Epoch 36/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9977 - val_loss: 3.0222\n",
      "Epoch 37/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9036 - val_loss: 2.9786\n",
      "Epoch 38/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9101 - val_loss: 2.8450\n",
      "Epoch 39/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0601 - val_loss: 2.8873\n",
      "Epoch 40/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0203 - val_loss: 2.8617\n",
      "Epoch 41/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9247 - val_loss: 2.8662\n",
      "Epoch 42/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9109 - val_loss: 2.9577\n",
      "Epoch 43/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0748 - val_loss: 2.8931\n",
      "Epoch 44/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9107 - val_loss: 2.8753\n",
      "Epoch 45/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0533 - val_loss: 2.8402\n",
      "Epoch 46/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9223 - val_loss: 2.8923\n",
      "Epoch 47/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9705 - val_loss: 2.8944\n",
      "Epoch 48/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0136 - val_loss: 2.8984\n",
      "Epoch 49/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8435 - val_loss: 2.8328\n",
      "Epoch 50/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0906 - val_loss: 2.8178\n",
      "Epoch 1/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.7093 - val_loss: 3.6071\n",
      "Epoch 2/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5497 - val_loss: 3.4701\n",
      "Epoch 3/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4398 - val_loss: 3.5557\n",
      "Epoch 4/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4985 - val_loss: 3.4242\n",
      "Epoch 5/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4378 - val_loss: 3.4892\n",
      "Epoch 6/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4052 - val_loss: 3.3409\n",
      "Epoch 7/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3882 - val_loss: 3.3555\n",
      "Epoch 8/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3856 - val_loss: 3.3858\n",
      "Epoch 9/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3887 - val_loss: 3.3805\n",
      "Epoch 10/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5238 - val_loss: 3.3041\n",
      "Epoch 11/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2866 - val_loss: 3.2959\n",
      "Epoch 12/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3846 - val_loss: 3.3189\n",
      "Epoch 13/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3888 - val_loss: 3.3004\n",
      "Epoch 14/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3222 - val_loss: 3.2684\n",
      "Epoch 15/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2751 - val_loss: 3.2995\n",
      "Epoch 16/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2777 - val_loss: 3.2865\n",
      "Epoch 17/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3309 - val_loss: 3.4038\n",
      "Epoch 18/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2238 - val_loss: 3.2527\n",
      "Epoch 19/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3504 - val_loss: 3.1929\n",
      "Epoch 20/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2347 - val_loss: 3.2190\n",
      "Epoch 21/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1381 - val_loss: 3.1739\n",
      "Epoch 22/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1804 - val_loss: 3.1779\n",
      "Epoch 23/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1054 - val_loss: 3.1285\n",
      "Epoch 24/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0957 - val_loss: 3.1850\n",
      "Epoch 25/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9750 - val_loss: 3.1048\n",
      "Epoch 26/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0840 - val_loss: 3.1684\n",
      "Epoch 27/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9437 - val_loss: 3.1680\n",
      "Epoch 28/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1248 - val_loss: 3.1258\n",
      "Epoch 29/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1653 - val_loss: 3.1643\n",
      "Epoch 30/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0703 - val_loss: 3.2697\n",
      "Epoch 31/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0723 - val_loss: 3.0542\n",
      "Epoch 32/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9626 - val_loss: 3.0415\n",
      "Epoch 33/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9657 - val_loss: 3.0791\n",
      "Epoch 34/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9286 - val_loss: 3.0242\n",
      "Epoch 35/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9656 - val_loss: 3.0499\n",
      "Epoch 36/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7930 - val_loss: 3.0928\n",
      "Epoch 37/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9991 - val_loss: 3.1258\n",
      "Epoch 38/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8841 - val_loss: 3.0523\n",
      "Epoch 39/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0161 - val_loss: 3.0635\n",
      "Epoch 40/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0315 - val_loss: 3.0150\n",
      "Epoch 41/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9257 - val_loss: 3.1798\n",
      "Epoch 42/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0620 - val_loss: 3.1037\n",
      "Epoch 43/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9795 - val_loss: 3.0187\n",
      "Epoch 44/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8889 - val_loss: 3.0463\n",
      "Epoch 45/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9395 - val_loss: 2.9908\n",
      "Epoch 46/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8475 - val_loss: 3.0469\n",
      "Epoch 47/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9229 - val_loss: 2.9984\n",
      "Epoch 48/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9307 - val_loss: 3.0012\n",
      "Epoch 49/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9254 - val_loss: 3.0104\n",
      "Epoch 50/50\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7903 - val_loss: 2.9953\n",
      "Average Training Loss: 2.8894564151763915\n",
      "Average Validation Loss: 2.9214437007904053\n"
     ]
    }
   ],
   "source": [
    "# Split features and target variable\n",
    "X = df.drop('secchi', axis=1)\n",
    "y = df['secchi']\n",
    "\n",
    "# Define the number of folds\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=37)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Grabbing this for later\n",
    "Random_lake = X.iloc[100]\n",
    "\n",
    "# Convert y to a numpy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Initialize lists to store the results from each fold\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Design the neural network\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='elu', input_shape=(X_train_fold.shape[1],)),\n",
    "        Dense(64, activation='elu'),\n",
    "        Dense(1)  # Output layer with a single neuron for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=custom_loss(delta=2.50))\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=30, validation_data=(X_val_fold, y_val_fold))\n",
    "    \n",
    "    # Evaluate the model on training and validation data\n",
    "    train_loss = model.evaluate(X_train_fold, y_train_fold, verbose=0)\n",
    "    val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    \n",
    "    # Append losses to lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Calculate average losses across folds\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "print(\"Average Training Loss:\", avg_train_loss)\n",
    "print(\"Average Validation Loss:\", avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing new test data for further evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Actual  Predicted\n",
      "43377    5.35   5.147142\n",
      "50285    4.90   5.585590\n",
      "44546    3.40   4.221447\n",
      "49371    4.60   3.806132\n",
      "22441    2.40   1.727856\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions with actual values:\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': predictions.flatten()})\n",
    "print(comparison.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Mean Percent Error: 22.794307666053374\n"
     ]
    }
   ],
   "source": [
    "comparison['Percent Error'] = (abs(comparison['Actual'] - comparison['Predicted']) / comparison['Actual']) * 100\n",
    "mean_percent_error = comparison['Percent Error'].mean()\n",
    "\n",
    "print(\"Absolute Mean Percent Error:\", mean_percent_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percent Error: 1.1615430065367869\n"
     ]
    }
   ],
   "source": [
    "comparison['Percent Error'] = ((comparison['Actual'] - comparison['Predicted']) / comparison['Actual']) * 100\n",
    "mean_percent_error = comparison['Percent Error'].mean()\n",
    "\n",
    "print(\"Mean Percent Error:\", mean_percent_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times Predicted is greater than Actual: 881\n",
      "Total number of predictions: 2214\n",
      "Percent of predictions clearer than actual: 39.79\n"
     ]
    }
   ],
   "source": [
    "count_predicted_greater_than_actual = (comparison['Predicted'] > comparison['Actual']).sum()\n",
    "total_predictions = len(comparison)\n",
    "\n",
    "print(\"Number of times Predicted is greater than Actual:\", count_predicted_greater_than_actual)\n",
    "print(\"Total number of predictions:\", total_predictions)\n",
    "print(\"Percent of predictions clearer than actual:\", round((count_predicted_greater_than_actual / total_predictions)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the model to predict values for an arbitrary lake\n",
    "\n",
    "Now that we have a model that predicts the secchi value from data about the lake, we can use this to examine how much human activity, specifically converting parts of a lake's watershed into farmland, a lake can handle.\n",
    "\n",
    "The initial idea was to frame it as an optimization problem and solve it through gradient descent. (Technically gradient ascent.) Basically, talk to the enviornmental scientists to get an idea of the minimum secchi value a lake's ecosystem could handle. Then we take the current composition of the watershed area and it's secchi value as our initial values. We can then use the model's predictions to power our cost function, we say yay to high argriculture and nay to secchi values under the target. This way, the steps that adjust the various percentage compositions of the watershed should be motivated to increase argicultural area as much as possible while still keeping lakes clean.\n",
    "\n",
    "This sounds really clever but ignores a massive issue. Firstly, we can't really just make more forest, grassland, etc and secondly, this ignores that the percentages must add up to 100. Without these constraints, there's nothing stopping us from having over 9000% forest to enable a huge farm near the lake. We have to provide the constraints that certain land types strictly decrease and that their percentages must add up to 100. The first one is easy, we just tack on a huge penalty to the cost function for increasing anything that shouldn't increase, no problem there. Well, no problem if we avoid getting trapped at local minimums though that can always be handled with more starting points and various step sizes at the cost of computation time.\n",
    "\n",
    "How about keeping all the percentages adding up to 100? In a 1987 paper \"Constrained Differential Optimization\", Platt and Barr provide a solution, the basic differential multiplier method.\n",
    "\n",
    "The method claims that for a Lagrangian: L(x, b) = f(x) + b g(x)\n",
    "\n",
    "Essentially, \"doing gradient descent on x while doing gradient 'ascend' on b, you will finally converge to a stationary point of L(x, b), which is a local minima of f(x) under the constraint g(x)=[100]. Penalty method could also be combined to make converge faster and stabler (which the authors call the modified differential multiplier method).\"\n",
    "\n",
    "\"Many optimization models of neural networks need constraints to restrict the space of outputs to a subspace which satisfies external criteria. Optimizations using energy methods yield \"forces\" which act upon the state of the neural network. The penalty method, in which quadratic energy constraints are added to an existing optimization energy, has become popular recently, but is not guaranteed to satisfy the constraint conditions when there are other forces on the neural model or when there are multiple constraints. In this paper, we present the basic differential multiplier method (BDMM), which satisfies constraints exactly; we create forces which gradually apply the constraints over time, using \"neurons\" that estimate Lagrange multipliers.\"\n",
    "\n",
    "https://stackoverflow.com/questions/12284638/gradient-descent-with-constraints-lagrange-multipliers\n",
    "https://papers.nips.cc/paper/1987/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html\n",
    "\n",
    "\n",
    "Okay wow, awesome, seems like we have this all figured out, through clever cost function design and multiple simultaneous ascents and descents we can build a model to optimize farmland. Very clever, also unfortunately somewhat pointless given the nature of the data and the nature of the real world. Firstly, we have no specific data on where any of the land actually is in relation to the lake, are the forests all together or are they in patches? Most often, we want to build a square farm so we wouldn't just patchwork it around the area to fit what a computer says is best. So, this initial suggestion of an optimizer is technically interesting in terms of implementation, thus the though experiment for personal satisfaction, but not usable given the limited data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Define lists to store perag, perfor, and secchi values for plotting\n",
    "perag_values = []\n",
    "perfor_values = []\n",
    "secchi_values = []\n",
    "\n",
    "Random_lake_copy = Random_lake.copy()  # Create a copy of the DataFrame\n",
    "\n",
    "\n",
    "# Fit the scaler to the original data once\n",
    "scaler.fit(Random_lake_copy.to_frame().transpose())\n",
    "\n",
    "# Loop through iterations\n",
    "for _ in range(25):\n",
    "    \n",
    "    # Transform the new data using the fitted scaler\n",
    "    Random_lake_scaled = scaler.transform(Random_lake_copy.to_frame().transpose())\n",
    "    \n",
    "    # Predict secchi using the scaled data\n",
    "    secchi = model.predict(Random_lake_scaled)[0][0]\n",
    "\n",
    "    # Append values to lists\n",
    "    perag_values.append(Random_lake_copy['perag'])\n",
    "    perfor_values.append(Random_lake_copy['perfor'])\n",
    "    secchi_values.append(secchi)\n",
    "\n",
    "    # Update perag and perfor\n",
    "    Random_lake_copy['perag'] += 0.2\n",
    "    Random_lake_copy['perfor'] -= 0.2\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(perag_values, secchi_values)\n",
    "plt.xlabel('Perag')\n",
    "plt.ylabel('Secchi')\n",
    "plt.title('Secchi vs Perag (Replacing Forest)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
